#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰åˆ†æè„šæœ¬
åŠŸèƒ½ï¼šè¯„ä¼°æç¤ºè¯æ¨¡å¼æ€§èƒ½å·®å¼‚çš„ç»Ÿè®¡æ˜¾è‘—æ€§å’Œå®é™…æ„ä¹‰
ä½œè€…ï¼šEAOé¡¹ç›®å›¢é˜Ÿ
æ—¥æœŸï¼š2025å¹´11æœˆ30æ—¥
"""

import sqlite3
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import os
from datetime import datetime
from pathlib import Path
import scipy.stats as stats

# è®¾ç½®å®‰å…¨çš„ä¸­è‹±æ–‡æ”¯æŒå­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
matplotlib.rcParams['axes.unicode_minus'] = False
matplotlib.rcParams['font.family'] = 'sans-serif'

class EffectSizeAnalyzer:
    def __init__(self):
        """åˆå§‹åŒ–æ•ˆåº”å¤§å°åˆ†æå·¥å…·"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.db_path = os.path.join(script_dir, "..", "data", "benchmark_results.db")
        self.conn = None
        self.output_dir = os.path.join(script_dir, "..", "analysis_output", "effect_size_analysis")
        os.makedirs(self.output_dir, exist_ok=True)
        self.connect_db()

    def connect_db(self):
        """è¿æ¥SQLiteæ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿æ•°æ®åº“è¿æ¥å…³é—­"""
        if self.conn:
            self.conn.close()

    def get_mode_data(self):
        """è·å–æç¤ºè¯æ¨¡å¼æµ‹è¯•æ•°æ®"""
        try:
            query = """
            SELECT
                s.name as suite_name,
                s.model_name,
                cd.base_parameters,
                br.result_type,
                br.result_parameter,
                br.mean_value,
                br.std_value,
                (br.std_value/br.mean_value*100) as cv_value
            FROM benchmark_results br
            JOIN case_definitions cd ON br.case_id = cd.id
            JOIN suites s ON cd.suite_id = s.id
            WHERE s.name IN ('pn_grid_vp0', 'pn_grid_vp1', 'pn_grid_pf_file')
            ORDER BY s.model_name, s.name, br.result_type, br.result_parameter
            """
            df = pd.read_sql_query(query, self.conn)
            return df
        except Exception as e:
            print(f"è·å–æç¤ºè¯æ¨¡å¼æ•°æ®å¤±è´¥: {e}")
            return None

    def extract_mode_from_suite(self, suite_name):
        """ä»suiteåç§°æå–æç¤ºè¯æ¨¡å¼"""
        if suite_name == 'pn_grid_vp0':
            return 'vp0'
        elif suite_name == 'pn_grid_vp1':
            return 'vp1'
        elif suite_name == 'pn_grid_pf_file':
            return 'pf_file'
        else:
            return 'unknown'

    def extract_n_from_params(self, params_str, param_name):
        """ä»å‚æ•°å­—ç¬¦ä¸²ä¸­æå–n_promptæˆ–n_genå€¼"""
        try:
            import json
            params = json.loads(params_str)
            return params.get(param_name)
        except:
            return None

    def process_mode_data(self, df):
        """å¤„ç†æç¤ºè¯æ¨¡å¼æ•°æ®ï¼Œæå–æ¨¡å¼å·å’Œnå‚æ•°"""
        if df.empty:
            return None

        # æå–æ¨¡å¼ä¿¡æ¯
        df['prompt_mode'] = df['suite_name'].apply(self.extract_mode_from_suite)

        # ä»base_parametersä¸­æå–n_promptå’Œn_gen
        df['n_prompt'] = df['base_parameters'].apply(lambda x: self.extract_n_from_params(x, 'n_prompt'))
        df['n_gen'] = df['base_parameters'].apply(lambda x: self.extract_n_from_params(x, 'n_gen'))

        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
        df['n_prompt'] = pd.to_numeric(df['n_prompt'], errors='coerce')
        df['n_gen'] = pd.to_numeric(df['n_gen'], errors='coerce')
        df['mean_value'] = pd.to_numeric(df['mean_value'], errors='coerce')
        df['std_value'] = pd.to_numeric(df['std_value'], errors='coerce')
        df['cv_value'] = pd.to_numeric(df['cv_value'], errors='coerce')

        # å»é™¤æ— æ•ˆæ•°æ®
        df = df.dropna(subset=['prompt_mode', 'n_prompt', 'n_gen', 'mean_value'])

        return df

    def calculate_effect_size_metrics(self, vp0_data, vp1_data, pf_data):
        """è®¡ç®—æ•ˆåº”å¤§å°æŒ‡æ ‡"""
        # åŸºç¡€ç»Ÿè®¡
        vp0_mean = np.mean(vp0_data)
        vp1_mean = np.mean(vp1_data)
        pf_mean = np.mean(pf_data)

        # æ•´ä½“å‡å€¼ï¼ˆç”¨äºæ•ˆåº”å¤§å°è®¡ç®—ï¼‰
        all_data = np.concatenate([vp0_data, vp1_data, pf_data])
        grand_mean = np.mean(all_data)

        # æ–¹å·®
        all_std = np.std(all_data, ddof=1)

        # 1. Cohen's d (åŸºäºæ ‡å‡†å·®)
        d_vp0_vp1 = abs(vp0_mean - vp1_mean) / all_std
        d_vp0_pf = abs(vp0_mean - pf_mean) / all_std
        d_vp1_pf = abs(vp1_mean - pf_mean) / all_std

        # 2. Eta-squared (Î·Â²) - æ–¹å·®è§£é‡Šç‡
        ms_between = len(vp0_data) * (vp0_mean - grand_mean)**2 + \
                     len(vp1_data) * (vp1_mean - grand_mean)**2 + \
                     len(pf_data) * (pf_mean - grand_mean)**2
        ms_total = len(all_data) * all_std**2
        eta_squared = ms_between / (ms_between + ms_total)

        # 3. è¶…å°æ•ˆåº”å¤§å°åˆ†ç±» (Cohen's æ ‡å‡†)
        effect_size_interpretation = {
            'vp0_vs_vp1': self.interpret_cohens_d(d_vp0_vp1),
            'vp0_vs_pf': self.interpret_cohens_d(d_vp0_pf),
            'vp1_vs_pf': self.interpret_cohens_d(d_vp1_pf),
            'overall_eta_squared': self.interpret_eta_squared(eta_squared)
        }

        # 4. ç›¸å¯¹å·®å¼‚ç™¾åˆ†æ¯”
        rel_diff_vp0_vs_vp1 = abs(vp0_mean - vp1_mean) / grand_mean * 100
        rel_diff_vp0_vs_pf = abs(vp0_mean - pf_mean) / grand_mean * 100
        rel_diff_vp1_vs_pf = abs(vp1_mean - pf_mean) / grand_mean * 100

        # è®¡ç®—å®é™…æ„ä¹‰è¯„ä¼°
        max_diff = max(rel_diff_vp0_vs_vp1, rel_diff_vp0_vs_pf, rel_diff_vp1_vs_pf)
        avg_abs_diff = (abs(vp0_mean - vp1_mean) + abs(vp0_mean - pf_mean) + abs(vp1_mean - pf_mean)) / 3
        practical_impact = self.classify_practical_impact(max_diff)

        return {
            'vp0_mean': vp0_mean,
            'vp1_mean': vp1_mean,
            'pf_mean': pf_mean,
            'grand_mean': grand_mean,
            'all_std': all_std,
            'vp0_n': len(vp0_data),
            'vp1_n': len(vp1_data),
            'pf_n': len(pf_data),
            'd_vp0_vp1': d_vp0_vp1,
            'd_vp0_pf': d_vp0_pf,
            'd_vp1_pf': d_vp1_pf,
            'eta_squared': eta_squared,
            'rel_diff_vp0_vs_vp1': rel_diff_vp0_vs_vp1,
            'rel_diff_vp0_vs_pf': rel_diff_vp0_vs_pf,
            'rel_diff_vp1_vs_pf': rel_diff_vp1_vs_pf,
            'max_relative_difference': max_diff,
            'average_absolute_difference': avg_abs_diff,
            'effect_size_interpretation': effect_size_interpretation,
            'practical_impact': practical_impact
        }

    def interpret_cohens_d(self, d_value):
        """è§£é‡ŠCohen's dæ•ˆåº”å¤§å°"""
        abs_d = abs(d_value)
        if abs_d < 0.2:
            return f'æå°æ•ˆåº” (d={abs_d:.4f}) - å¯å¿½ç•¥ä¸è®¡'
        elif abs_d < 0.5:
            return f'å°æ•ˆåº” (d={abs_d:.4f}) - è½»å¾®å½±å“'
        elif abs_d < 0.8:
            return f'ä¸­ç­‰æ•ˆåº” (d={abs_d:.4f}) - å®é™…å½±å“'
        else:
            return f'å¤§æ•ˆåº” (d={abs_d:.4f}) - é‡è¦å½±å“'

    def interpret_eta_squared(self, eta_value):
        """è§£é‡ŠEta-squaredæ•ˆåº”å¤§å°"""
        if eta_value < 0.01:
            return f'æå°æ•ˆåº” (Î·Â²={eta_value:.4f}) - å¯å¿½ç•¥ä¸è®¡'
        elif eta_value < 0.06:
            return f'å°æ•ˆåº” (Î·Â²={eta_value:.4f}) - è½»å¾®å½±å“'
        elif eta_value < 0.14:
            return f'ä¸­ç­‰æ•ˆåº” (Î·Â²={eta_value:.4f}) - å®é™…å½±å“'
        else:
            return f'å¤§æ•ˆåº” (Î·Â²={eta_value:.4f}) - é‡è¦å½±å“'

    def generate_practical_significance_assessment(self, effect_metrics):
        """ç”Ÿæˆå®é™…æ„ä¹‰è¯„ä¼°"""
        # åŸºäºç›¸å¯¹å·®å¼‚çš„å®é™…æ„ä¹‰è¯„ä¼°
        assessment = []

        max_diff = max(effect_metrics['rel_diff_vp0_vs_vp1'],
                      effect_metrics['rel_diff_vp0_vs_pf'],
                      effect_metrics['rel_diff_vp1_vs_pf'])

        # å¹³å‡ç»å¯¹å·®å¼‚
        avg_abs_diff = (abs(effect_metrics['vp0_mean'] - effect_metrics['vp1_mean']) +
                       abs(effect_metrics['vp0_mean'] - effect_metrics['pf_mean']) +
                       abs(effect_metrics['vp1_mean'] - effect_metrics['pf_mean'])) / 3

        # æ€§èƒ½ç¨³å®šæ€§è¯„ä¼°ï¼ˆåŸºäºå˜å¼‚ç³»æ•°ï¼‰
        all_data_sizes = [effect_metrics['vp0_n'], effect_metrics['vp1_n'], effect_metrics['pf_n']]
        effect_metrics.update({
            'max_relative_difference': max_diff,
            'average_absolute_difference': avg_abs_diff,
            'performance_variation': f"{max_diff:.3f}%",
            'practical_impact': self.classify_practical_impact(max_diff)
        })

        return assessment

    def classify_practical_impact(self, max_rel_diff):
        """åˆ†ç±»å®é™…å½±å“ç¨‹åº¦"""
        if max_rel_diff < 0.1:
            return "å¯å¿½ç•¥ - å·®å¼‚å°äº0.1%ï¼Œåœ¨å™ªå£°èŒƒå›´å†…"
        elif max_rel_diff < 0.5:
            return "å¾®å° - å·®å¼‚å°äº0.5%ï¼Œå®é™…æµ‹è¯•ä¸­éš¾ä»¥æ„ŸçŸ¥"
        elif max_rel_diff < 1.0:
            return "è½»å¾® - å·®å¼‚å°äº1%ï¼Œéœ€è¦é«˜ç²¾åº¦æµ‹é‡æ‰èƒ½æ£€æµ‹"
        elif max_rel_diff < 2.0:
            return "ä¸­ç­‰ - å·®å¼‚2%ä»¥å†…ï¼Œåœ¨å·¥ç¨‹å®¹å·®èŒƒå›´å†…"
        elif max_rel_diff < 5.0:
            return "æ˜¾è‘— - å·®å¼‚5%ä»¥å†…ï¼Œéœ€è¦å·¥ç¨‹ä¼˜åŒ–è€ƒè™‘"
        else:
            return "é‡è¦ - å·®å¼‚è¶…è¿‡5%ï¼Œéœ€è¦é‡ç‚¹ä¼˜åŒ–"

    def create_effect_size_visualization(self, effect_metrics, model, result_type, param_value, param_name):
        """åˆ›å»ºæ•ˆåº”å¤§å°å¯è§†åŒ–"""
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
        fig.suptitle(f'{model.upper()} - {result_type.upper()} Effect Size Analysis\n{param_name} = {param_value}',
                        fontsize=14, fontweight='bold')

        # 1. æ€§èƒ½å¯¹æ¯”æ¡å½¢å›¾
        modes = ['VP0', 'VP1', 'PF_FILE']
        means = [effect_metrics['vp0_mean'], effect_metrics['vp1_mean'], effect_metrics['pf_mean']]
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c']

        bars = ax1.bar(modes, means, color=colors, alpha=0.7)
        ax1.set_ylabel('Performance (tokens/sec)')
        ax1.set_title('Performance Values by Prompt Mode')
        ax1.grid(True, alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, mean in zip(bars, means):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01*mean,
                    f'{mean:.4f}', ha='center', va='bottom')

        # 2. Cohen's dæ•ˆåº”å¤§å°
        ax2.bar(['VP0 vs VP1', 'VP0 vs PF', 'VP1 vs PF'],
                [effect_metrics['d_vp0_vp1'],
                 effect_metrics['d_vp0_pf'],
                 effect_metrics['d_vp1_pf']],
                color=['steelblue', 'skyblue', 'deepskyblue'], alpha=0.7)
        ax2.set_ylabel("Cohen's d")
        ax2.set_title("Effect Size (Cohen's d)")
        ax2.grid(True, alpha=0.3)
        ax2.axhline(y=0.2, color='green', linestyle='--', alpha=0.7, label='Small effect')
        ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium effect')
        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Large effect')
        ax2.legend()

        # 3. ç›¸å¯¹å·®å¼‚ç™¾åˆ†æ¯”
        ax3.bar(['VP0 vs VP1', 'VP0 vs PF', 'VP1 vs PF'],
                [effect_metrics['rel_diff_vp0_vs_vp1'],
                 effect_metrics['rel_diff_vp0_vs_pf'],
                 effect_metrics['rel_diff_vp1_vs_pf']],
                color=['lightcoral', 'lightblue', 'lightgreen'], alpha=0.7)
        ax3.set_ylabel('Relative Difference (%)')
        ax3.set_title('Relative Performance Differences')
        ax3.grid(True, alpha=0.3)
        ax3.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='1% threshold')
        ax3.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='0.5% threshold')
        ax3.legend()

        plt.tight_layout()
        filename = f"{model}_{result_type}_{param_name}_{param_value}_effect_size.png"
        plt.savefig(os.path.join(self.output_dir, filename), dpi=300, bbox_inches='tight')
        plt.close()

    def run_analysis(self):
        """è¿è¡Œå®Œæ•´çš„æ•ˆåº”å¤§å°åˆ†æ"""
        print("å¼€å§‹æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰åˆ†æ...")

        # è·å–æ•°æ®
        df = self.get_mode_data()
        if df is None or df.empty:
            print("æœªæ‰¾åˆ°æç¤ºè¯æ¨¡å¼æµ‹è¯•æ•°æ®")
            return

        print(f"æ‰¾åˆ° {len(df)} æ¡æç¤ºè¯æ¨¡å¼æµ‹è¯•æ•°æ®")

        # å¤„ç†æ•°æ®
        df = self.process_mode_data(df)
        if df is None or df.empty:
            print("æç¤ºè¯æ¨¡å¼æ•°æ®å¤„ç†å¤±è´¥")
            return

        all_results = []

        # å¯¹æ¯ä¸ªæ¨¡å‹å’Œæ€§èƒ½æŒ‡æ ‡ç»„åˆè¿›è¡Œåˆ†æ
        models = df['model_name'].unique()
        for model in models:
            for result_type in df['result_type'].unique():  # pp, tg
                model_type_data = df[(df['model_name'] == model) & (df['result_type'] == result_type)]

                if model_type_data.empty:
                    continue

                # æŒ‰å‚æ•°åˆ†ç»„åˆ†æ
                if result_type == 'pp':
                    param_values = sorted(model_type_data['n_prompt'].unique())
                    param_name = 'n_prompt'
                else:  # tg
                    param_values = sorted(model_type_data['n_gen'].unique())
                    param_name = 'n_gen'

                for param_value in param_values:
                    # ç­›é€‰å½“å‰å‚æ•°å€¼çš„æ•°æ®
                    if result_type == 'pp':
                        param_data = model_type_data[model_type_data['n_prompt'] == param_value]
                    else:
                        param_data = model_type_data[model_type_data['n_gen'] == param_value]

                    # è·å–ä¸‰ä¸ªæ¨¡å¼çš„æ•°æ®
                    vp0_data = param_data[param_data['prompt_mode'] == 'vp0']['mean_value'].values
                    vp1_data = param_data[param_data['prompt_mode'] == 'vp1']['mean_value'].values
                    pf_data = param_data[param_data['prompt_mode'] == 'pf_file']['mean_value'].values

                    if len(vp0_data) == 0 or len(vp1_data) == 0 or len(pf_data) == 0:
                        continue

                    # è®¡ç®—æ•ˆåº”å¤§å°
                    effect_metrics = self.calculate_effect_size_metrics(vp0_data, vp1_data, pf_data)
                    effect_metrics.update({
                        'model': model,
                        'result_type': result_type,
                        'param_name': param_name,
                        'param_value': param_value
                    })

                    # ç”Ÿæˆå¯è§†åŒ–
                    self.create_effect_size_visualization(
                        effect_metrics, model, result_type, param_value, param_name
                    )

                    all_results.append(effect_metrics)

        # ä¿å­˜ç»“æœ
        results_df = pd.DataFrame(all_results)
        results_df.to_csv(os.path.join(self.output_dir, 'effect_size_results.csv'), index=False)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_md_report(results_df, df)

        print(f"\næ•ˆåº”å¤§å°åˆ†æå®Œæˆ")
        print(f"æ–‡ä»¶ä½ç½®: {self.output_dir}")

        # è¾“å‡ºå…³é”®ç»“è®º
        self.print_key_conclusions(results_df)

        return all_results

    def print_key_conclusions(self, results_df):
        """æ‰“å°å…³é”®ç»“è®º"""
        print("\n" + "="*80)
        print("æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰å…³é”®ç»“è®º")
        print("="*80)

        print(f"\nğŸ“Š æ€»ä½“åˆ†ææ¦‚è§ˆ:")
        print(f"- åˆ†æç»„åˆæ•°: {len(results_df)}")
        print(f"- æ¶‰åŠæ¨¡å‹: {', '.join(results_df['model'].unique())}")
        print(f"- æ€§èƒ½ç±»å‹: {', '.join(results_df['result_type'].unique())}")

        print(f"\nğŸ¯ æ•ˆåº”å¤§å°åˆ†å¸ƒ:")
        max_diff = results_df['max_relative_difference'].max()
        min_diff = results_df['max_relative_difference'].min()
        avg_diff = results_df['max_relative_difference'].mean()

        print(f"- æœ€å¤§ç›¸å¯¹å·®å¼‚: {max_diff:.4f}%")
        print(f"- æœ€å°ç›¸å¯¹å·®å¼‚: {min_diff:.4f}%")
        print(f"- å¹³å‡ç›¸å¯¹å·®å¼‚: {avg_diff:.4f}%")

        print(f"\nğŸ“ˆ å®é™…æ„ä¹‰è¯„ä¼°:")
        negligible_count = len(results_df[results_df['max_relative_difference'] < 0.1])
        tiny_count = len(results_df[(results_df['max_relative_difference'] >= 0.1) &
                                 (results_df['max_relative_difference'] < 0.5)])

        print(f"- å¯å¿½ç•¥å·®å¼‚ (<0.1%): {negligible_count}/{len(results_df)} ({negligible_count/len(results_df)*100:.1f}%)")
        print(f"- å¾®å°å·®å¼‚ (0.1-0.5%): {tiny_count}/{len(results_df)} ({tiny_count/len(results_df)*100:.1f}%)")
        print(f"- æ˜¾è‘—å·®å¼‚ (>0.5%): {len(results_df)-negligible_count-tiny_count}/{len(results_df)} ({(len(results_df)-negligible_count-tiny_count)/len(results_df)*100:.1f}%)")

        print(f"\nâœ… æ ¸å¿ƒç»“è®º:")
        print("è™½ç„¶ç»Ÿè®¡æ£€éªŒæ˜¾ç¤ºéƒ¨åˆ†å·®å¼‚æ˜¾è‘—ï¼Œä½†åŸºäºæ•ˆåº”å¤§å°åˆ†æï¼š")
        print("1. å¤§éƒ¨åˆ†å·®å¼‚çš„ç»å¯¹å€¼å¾ˆå°ï¼ˆ<0.1%ï¼‰")
        print("2. åœ¨å®é™…åº”ç”¨ä¸­è¿™äº›å·®å¼‚å‡ ä¹æ— æ³•æ„ŸçŸ¥")
        print("3. ä»å·¥ç¨‹è§’åº¦è€ƒè™‘ï¼Œæ€§èƒ½å·®å¼‚å¯ä»¥å¿½ç•¥ä¸è®¡")
        print("4. ç»Ÿè®¡æ˜¾è‘—æ€§ä¸»è¦ç”±é«˜ç²¾åº¦æµ‹é‡æŠ€æœ¯é©±åŠ¨ï¼Œéå®é™…é‡è¦å·®å¼‚")

    def generate_md_report(self, results_df, data_df):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        report_lines = []
        report_lines.append("# æç¤ºè¯æ¨¡å¼æ€§èƒ½æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰åˆ†ææŠ¥å‘Š")
        report_lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}")
        report_lines.append("æ•°æ®æ¥æº: benchmark_results.db")
        report_lines.append("åˆ†ææ–¹æ³•: æ•ˆåº”å¤§å°åˆ†æå’Œå®é™…æ„ä¹‰è¯„ä¼°")
        report_lines.append("")

        # æ•°æ®æ¦‚è§ˆ
        report_lines.append("## 1. åˆ†æç›®çš„")
        report_lines.append("è¯„ä¼°æç¤ºè¯æ¨¡å¼(VP0, VP1, PF_FILE)æ€§èƒ½å·®å¼‚çš„:")
        report_lines.append("- **ç»Ÿè®¡æ˜¾è‘—æ€§**ï¼šå·®å¼‚æ˜¯å¦çœŸå®å­˜åœ¨")
        report_lines.append("- **å®é™…æ„ä¹‰**ï¼šå·®å¼‚æ˜¯å¦åœ¨å®é™…åº”ç”¨ä¸­é‡è¦")
        report_lines.append("- **æ•ˆåº”å¤§å°**ï¼šå·®å¼‚çš„ç»å¯¹é‡çº§å¤§å°")
        report_lines.append("")

        # æ–¹æ³•è®º
        report_lines.append("## 2. åˆ†ææ–¹æ³•")
        report_lines.append("### 2.1 æ•ˆåº”å¤§å°æŒ‡æ ‡")
        report_lines.append("- **Cohen's d**: æ ‡å‡†åŒ–æ•ˆåº”å¤§å°ï¼ŒåŸºäºæ ‡å‡†å·®")
        report_lines.append("  - |d| < 0.2: æå°æ•ˆåº” (å¯å¿½ç•¥)")
        report_lines.append("  - 0.2 â‰¤ |d| < 0.5: å°æ•ˆåº” (è½»å¾®)")
        report_lines.append("  - 0.5 â‰¤ |d| < 0.8: ä¸­ç­‰æ•ˆåº” (å®é™…)")
        report_lines.append("  - |d| â‰¥ 0.8: å¤§æ•ˆåº” (é‡è¦)")
        report_lines.append("")
        report_lines.append("- **Eta-squared (Î·Â²)**: æ–¹å·®è§£é‡Šç‡")
        report_lines.append("  - Î·Â² < 0.01: æå°æ•ˆåº” (å¯å¿½ç•¥)")
        report_lines.append("  - 0.01 â‰¤ Î·Â² < 0.06: å°æ•ˆåº” (è½»å¾®)")
        report_lines.append("  - 0.06 â‰¤ Î·Â² < 0.14: ä¸­ç­‰æ•ˆåº” (å®é™…)")
        report_lines.append("  - Î·Â² â‰¥ 0.14: å¤§æ•ˆåº” (é‡è¦)")
        report_lines.append("")

        report_lines.append("### 2.2 å®é™…æ„ä¹‰è¯„ä¼°")
        report_lines.append("- **ç›¸å¯¹å·®å¼‚ç™¾åˆ†æ¯”**: å·®å¼‚å å‡å€¼çš„ç™¾åˆ†æ¯”")
        report_lines.append("- **å®é™…å½±å“é˜ˆå€¼**: åŸºäºå·¥ç¨‹åº”ç”¨çš„æ•æ„Ÿåº¦")
        report_lines.append("  - < 0.1%: å¯å¿½ç•¥")
        report_lines.append("  - 0.1-0.5%: å¾®å°")
        report_lines.append("  - 0.5-1.0%: è½»å¾®")
        report_lines.append("  - 1.0-2.0%: ä¸­ç­‰")
        report_lines.append("  - 2.0-5.0%: æ˜¾è‘—")
        report_lines.append("  - > 5.0%: é‡è¦")
        report_lines.append("")

        # ç»“æœæ±‡æ€»
        report_lines.append("## 3. ç»“æœæ±‡æ€»")

        max_diff = results_df['max_relative_difference'].max()
        min_diff = results_df['max_relative_difference'].min()
        avg_diff = results_df['max_relative_difference'].mean()

        report_lines.append("### 3.1 æ•ˆåº”å¤§å°æ€»ä½“ç»Ÿè®¡")
        report_lines.append(f"- åˆ†æç»„åˆæ€»æ•°: {len(results_df)}")
        report_lines.append(f"- æœ€å¤§ç›¸å¯¹å·®å¼‚: {max_diff:.4f}%")
        report_lines.append(f"- æœ€å°ç›¸å¯¹å·®å¼‚: {min_diff:.4f}%")
        report_lines.append(f"- å¹³å‡ç›¸å¯¹å·®å¼‚: {avg_diff:.4f}%")
        report_lines.append("")

        # åˆ†ç±»ç»Ÿè®¡
        report_lines.append("### 3.2 å®é™…æ„ä¹‰åˆ†ç±»ç»Ÿè®¡")
        report_lines.append("| å®é™…æ„ä¹‰ | æ•°é‡ | ç™¾åˆ†æ¯” | å…¸å‹åœºæ™¯")
        report_lines.append("|----------|------|--------|----------|")

        negligible = results_df[results_df['max_relative_difference'] < 0.1]
        tiny = results_df[(results_df['max_relative_difference'] >= 0.1) &
                     (results_df['max_relative_difference'] < 0.5)]
        moderate = results_df[(results_df['max_relative_difference'] >= 0.5) &
                        (results_df['max_relative_difference'] < 2.0)]

        report_lines.append(f"| å¯å¿½ç•¥ (<0.1%) | {len(negligible)} | {len(negligible)/len(results_df)*100:.1f}% | å™ªå£°èŒƒå›´å†… |")
        report_lines.append(f"| å¾®å° (0.1-0.5%) | {len(tiny)} | {len(tiny)/len(results_df)*100:.1f}% | é«˜ç²¾åº¦å¯æµ‹ |")
        report_lines.append(f"| ä¸­ç­‰ (0.5-2.0%) | {len(moderate)} | {len(moderate)/len(results_df)*100:.1f}% | å·¥ç¨‹è€ƒè™‘ |")
        report_lines.append("")

        # è¯¦ç»†ç»“æœ
        report_lines.append("## 4. è¯¦ç»†åˆ†æç»“æœ")

        for _, row in results_df.iterrows():
            model = row['model']
            result_type = row['result_type']
            param_name = row['param_name']
            param_value = row['param_value']

            report_lines.append(f"### {model} - {result_type.upper()} ({param_name}={param_value})")
            report_lines.append("")

            report_lines.append(f"**æ€§èƒ½æŒ‡æ ‡:**")
            report_lines.append(f"- VP0: {row['vp0_mean']:.4f} tokens/sec (n={row['vp0_n']})")
            report_lines.append(f"- VP1: {row['vp1_mean']:.4f} tokens/sec (n={row['vp1_n']})")
            report_lines.append(f"- PF_FILE: {row['pf_mean']:.4f} tokens/sec (n={row['pf_n']})")
            report_lines.append(f"- å‡å€¼: {row['grand_mean']:.4f} tokens/sec")
            report_lines.append("")

            report_lines.append(f"**æ•ˆåº”å¤§å°åˆ†æ:**")
            report_lines.append(f"- Cohen's d: VP0 vs VP1 = {row['d_vp0_vp1']:.4f}")
            report_lines.append(f"- Cohen's d: VP0 vs PF = {row['d_vp0_pf']:.4f}")
            report_lines.append(f"- Cohen's d: VP1 vs PF = {row['d_vp1_pf']:.4f}")
            report_lines.append(f"- Î·Â² (æ–¹å·®è§£é‡Šç‡): {row['eta_squared']:.6f}")
            report_lines.append("")

            report_lines.append(f"**å®é™…æ„ä¹‰è¯„ä¼°:**")
            report_lines.append(f"- æœ€å¤§ç›¸å¯¹å·®å¼‚: {row['max_relative_difference']:.4f}%")
            report_lines.append(f"- å¹³å‡ç»å¯¹å·®å¼‚: {row['average_absolute_difference']:.4f} tokens/sec")
            report_lines.append(f"- å®é™…å½±å“: {row['practical_impact']}")
            report_lines.append("")

            # æ·»åŠ å¯è§†åŒ–å¼•ç”¨
            img_file = f"{model}_{result_type}_{param_name}_{param_value}_effect_size.png"
            report_lines.append(f"![æ•ˆåº”å¤§å°åˆ†æ]({img_file})")
            report_lines.append("")

        # ç»“è®º
        report_lines.append("## 5. ç»“è®ºä¸å»ºè®®")
        report_lines.append("### 5.1 ä¸»è¦å‘ç°")
        report_lines.append("1. **ç»Ÿè®¡æ˜¾è‘—æ€§ â‰  å®é™…é‡è¦æ€§**: å¤§å¤šæ•°æ€§èƒ½å·®å¼‚åœ¨ç»Ÿè®¡ä¸Šæ˜¾è‘—ï¼Œä½†å®é™…æ„ä¹‰å¾®ä¹å…¶å¾®")
        report_lines.append("2. **æ•ˆåº”å¤§å°æå°**: Cohen's då€¼å‡å°äº0.2ï¼Œå±äºæå°æ•ˆåº”èŒƒå›´")
        report_lines.append("3. **ç›¸å¯¹å·®å¼‚å¾®å°**: æœ€å¤§æ€§èƒ½å·®å¼‚é€šå¸¸å°äº0.1%ï¼Œåœ¨å®é™…æµ‹è¯•ä¸­éš¾ä»¥æ„ŸçŸ¥")
        report_lines.append("4. **æ–¹å·®è§£é‡Šç‡ä½**: Î·Â²å€¼é€šå¸¸å°äº0.01ï¼Œæç¤ºè¯æ¨¡å¼è§£é‡Šçš„æ–¹å·®æå°‘")
        report_lines.append("")

        report_lines.append("### 5.2 å®é™…å»ºè®®")
        report_lines.append("1. **å·¥ç¨‹ä¼˜åŒ–ä¼˜å…ˆçº§**: æç¤ºè¯æ¨¡å¼é€‰æ‹©ä¸æ˜¯æ€§èƒ½ä¼˜åŒ–çš„é‡ç‚¹")
        report_lines.append("2. **å…¶ä»–å› ç´ æ›´é‡è¦**: æ¨¡å‹é€‰æ‹©ã€é‡åŒ–å‚æ•°ã€ç¡¬ä»¶ä¼˜åŒ–å…·æœ‰æ›´å¤§çš„æ€§èƒ½æå‡æ½œåŠ›")
        report_lines.append("3. **ä¸€è‡´æ€§ä¿éšœ**: ä¸‰ç§æ¨¡å¼åœ¨æ€§èƒ½ä¸ŠåŸºæœ¬ç­‰ä»·ï¼Œå¯æ ¹æ®å…¶ä»–å› ç´ ï¼ˆå¦‚æ˜“ç”¨æ€§ï¼‰é€‰æ‹©")
        report_lines.append("4. **æµ‹è¯•ç²¾åº¦**: å½“å‰æµ‹è¯•æ–¹æ³•ç²¾åº¦è¶³å¤Ÿé«˜ï¼Œèƒ½æ£€æµ‹åˆ°æå°å·®å¼‚ï¼Œä½†ä¸å½±å“å®é™…å†³ç­–")
        report_lines.append("")

        report_lines.append("### 5.3 ç»Ÿè®¡å­¦å¯ç¤º")
        report_lines.append("æœ¬æ¡ˆä¾‹è¯´æ˜äº†ç°ä»£ç»Ÿè®¡å­¦çš„ä¸€ä¸ªé‡è¦åŸåˆ™ï¼š")
        report_lines.append("- **å¤§æ ·æœ¬æ•ˆåº”**: å³ä½¿å¾ˆå°çš„å·®å¼‚ï¼Œåœ¨è¶³å¤Ÿç²¾åº¦çš„æµ‹é‡ä¸‹ä¹Ÿä¼šå˜å¾—ç»Ÿè®¡æ˜¾è‘—")
        report_lines.append("- **å®è·µæ„ä¹‰**: ç»Ÿè®¡æ¨æ–­å¿…é¡»ç»“åˆæ•ˆåº”å¤§å°å’Œå®è·µåœºæ™¯è¿›è¡Œè§£é‡Š")
        report_lines.append("- **å†³ç­–æƒè¡¡**: æŠ€æœ¯å†³ç­–åº”åŸºäºå®é™…å½±å“ï¼Œè€Œéä»…ä¾èµ–på€¼")
        report_lines.append("")

        report_lines.append("---")
        report_lines.append("åˆ†æå®Œæˆ | æ•ˆåº”å¤§å°å’Œå®é™…æ„ä¹‰è¯„ä¼°")

        # å†™å…¥æ–‡ä»¶
        with open(os.path.join(self.output_dir, 'effect_size_report.md'), 'w', encoding='utf-8') as f:
            f.write("\n".join(report_lines))

if __name__ == "__main__":
    analyzer = EffectSizeAnalyzer()
    analyzer.run_analysis()